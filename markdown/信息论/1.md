# 各种熵
$$
\begin{aligned}
    &(1)&H(X)&=-\sum_ip(x_i)\log_2p(x_i)\\
    &(2)&H(Y|x_i)&=-\sum_jp(y_j|x_i)\log_2p(y_j|x_i)\\
   &(3)& H(Y|X)&=\sum_i\underline{\color{red}p(x_i)}H(Y|x_i)\\
    &&&=-\sum_i\sum_jp(x_i,y_j)\log_2p(y_j|x_i)\\
    &(4)&H(X,Y)&=-\sum_i\sum_jp(x_i,y_j)\log_2p(y_j,x_i)\\
    &(5)&I(Y;x_i)&=\sum_jp(y_j|x_i)\log_2\frac{p(y_j|x_i)}{p(y_i)}\\
    &(6)&I(Y;X)&=\sum_i\underline{\color{red}p(x_i)}I(Y;x_i)\\
    &&&=\sum_i\sum_jp(x_i,y_j)\log_2\frac{p(y_j|x_i)}{p(y_i)}
\end{aligned}
$$
# 概念区分
## 非奇异码

## 唯一可译码

## 即时码
如何判别一个**唯一可译码**是否是即时码？
> 唯一可译码中的任意一个码字不是其他码字的前缀，则其是即时码.


## 信源符号、码符号、码元、码字
1. 信源符号
2. 码符号

## 信源剩余度
$$
1-\frac{H_\infty}{H_0}
$$
其中，$H_0$是等概分布时的最大熵$\log_2q$.
# 扩展信源及其编码
注意是无记忆扩展！
$$
\overline{L}=\frac{\overline{L_N}}{N}\\
H_N(S)=\frac{H(S^N)}{N}
$$
分别对应着单个信源时的平均码长$\overline{L}$和熵$H(S)$.  

---
$\overline{L}$下限：
$$\displaystyle\overline{L}\ge{}\frac{H_N(S)}{\log_2r}\left(=\frac{H(S)}{\log_2r},当为无记忆扩展信源\right)$$

当$N\rightarrow\infty$时，根据香农第一定理，有
$$\displaystyle\lim_{N\rightarrow{}\infty}\overline{L}={\frac{H(S)}{\log_2r}}$$

注意这里使用的时$H(S)$，即原始信源的熵。

如果是**有记忆扩展**，那么应该使用$H_{\infty}(S)$:
$$
\lim_{N\rightarrow{}\infty}\overline{L}={\frac{H_\infty(S)}{\log_2r}}
$$

## $H(S)$和$H_{\infty}(S)$


## 编码速率和效率
编码速率(rate)：
$$
R=\overline{L}\log_2r
$$
编码效率：
$$
\eta=\frac{H(S)}{R}
$$
信源剩余度：
$$
\gamma=1-\eta
$$


## 定长码的所需长度
$$
N\ge{}\frac{D[I(s_i)]}{H^2(S)}\frac{\eta^2}{(1-\eta)^2\delta}
$$
其中$D[I(s_i)]$为自信息的方差，计算如下
$$
D[I(s_i)]=\sum_ip(s_i)[\log_2p(s_i)]^2-[H(S)]^2
$$
$\delta$为允许的错误概率上限，比如$\delta\le{}10^{-6}$.
$\eta$为编码效率.

# 从单位看性质
## 平均码元长度：
$$
\overline{L}=\sum_ip(s_i)l_i\qquad{}码元/符号
$$
即：每个符号$s_i$编码所需的码元个数（的平均）为$\overline{L}$.
## 信源编码速率
$$
R=\overline{L}\log_2r\qquad{}bit/码元
$$
即：每个码元所携带的信息量.
## 信源编码效率

---
## 信道的信息传输率和信道容量
！注意：以上两个概念都是信源编码的理论，这个是信道的特性！

$$
\begin{aligned}
信息传输率\qquad R&=I(X;Y)\qquad{}&bit/符号\\
信道容量\qquad C&=\max_{p(X)}\{I(X;Y)\}\qquad{}&bit/符号    
\end{aligned}
$$
$I(X;Y)$的含义是：接收到符号$Y$后，**平均**每个接收到的**符号**获得的关于$X$的**信息量**，因此信道的信息传输率就是平均互信息

## 一个题目
已知信道传输矩阵$P$
$$
P=\begin{pmatrix}
    \dfrac{3}{4}&\dfrac{1}{8}&\dfrac{1}{8}\\\\
    \dfrac{1}{8}&\dfrac{1}{8}&\dfrac{3}{4}
\end{pmatrix}
$$
输入信源符号分布为
$$
P_X=\begin{pmatrix}
    0.8&0.2
\end{pmatrix}
$$
现有10000个信源符号，以1500符号/秒的速度传输，问20秒能否传送完所有信息？

**解**：

# 信道容量

## 常见信道的信道容量求解
### 对称信道
$$
C = \log_2r-H(p_1,p_2,\cdots,p_r)
$$
其中$r$为输出符号的个数。

---
### 输入对称信道
**定义**    
各**行**之间互为排列.

> 考试的时候**一般**都是是特殊的准对称信道，输入等概时为最大   
> 当然并不是都是准对称信道

1. 求输入$X$等概时的$H(Y)$
2. 不必求$H(Y|X)$，由于每行在求熵的时候都是一样的，推算后发现，$H(Y|X)=H(p_1,p_2,\cdots,p_r)$
3. 相减即得

如果不是准对称，应使用公式
$$
C=\max_{p(X)}\{H(Y)\}-H(p_1,p_2,\cdots,p_r)
$$

### 准对称信道
**定义**   
按输出划分（即竖着切）成n个子矩阵，这些子矩阵的行之间互为排列， 列之间也互为排列。

> 输入等概时的信息传输速率$I(X;Y)$即为信道容量

1. 求输入$X$等概时的$H(Y)$
2. 求$H(Y|X)$
3. $C=I(X;Y)=H(Y)-H(Y|X)$

### 确定信道
信道矩阵每行有且只有一个1.

意味着，每个发送的信源符号必定有唯一确定的信宿符号。这意味着$H(Y|X)=0$————知道X必知道Y.因此：
$$
C=\log_2r
$$
### 无损信道
每个信源符号都有对应的信宿，但可能对应多个（互不相交）的新宿符号。

与一般信道不同的是，一旦接收到符号$Y$后，必可知道发送信号，即$H(X|Y)=0$,因此信道容量$I(X;Y)$的最大值就是信源熵的最大值
$$
C=\log_2s
$$