对于深层的网络，权重(向量)$w$会有十分深远的影响。

对于$w$来说，计算的过程是每一层都要乘以它，经过多层以后，这种效果会十分巨大（指数级？）。

而激活函数例如($sigmoid(z)$)函数来说，$z$过大会导致梯度十分缓慢，这就是梯度消失或爆炸

一个经验性的方法是，通过这样一个不那么随机的初始化方法（有卵用？）：

$$w^{[l]} = np.random.randn(shape)*np.sqrt(1/n^{[l-1]})$$